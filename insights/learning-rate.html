<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>CP191124 | Aryan Ankolekar</title>
  <link rel="icon" type="image/png" href="assets/f1.png" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" />
  <link rel="stylesheet" href="styles.css" />

</head>
<body>
  <div class="page-wrapper">
    <div class="main-content">
      <header>
        <h1><img src="assets/f1.png" alt="favicon" class="favicon" />Aryan Ankolekar</h1>
        <div class="nav-links">
          <a href="../index.html">Home</a>
          <a href="../about.html">About</a>
          <a href="insights.html">Insights</a>
        </div>
      </header> 

      <div class="blog-tags">
        <span class="tag">Machine Learning</span>
        <span class="tag">Optimization</span>
        <span class="tag">Deep Learning</span>
        <span class="tag">Neural Networks</span>
      </div>

      <section id="title">
        <h1 class="text-4xl font-bold text-yellow-600 mb-6">CP191124. The Art of Making Models Learn: Optimizers, Schedulers & MobileNetV3</h1>
        <p>At its core, training a neural network means minimizing a loss function. This loss measures how far off your predictions are from the actual labels. The goal of an optimizer is to adjust the model's parameters (weights and biases) in a way that this loss is minimized.</p>
      </section>


      <section id="intro">
        <h2>Introduction</h2>
        <p>Training deep learning models isn't just about throwing data at a neural network and waiting for magic. It's a finely tuned balance of algorithms, parameters, and timing. Among these, optimizers and learning rate schedulers play a crucial role in determining whether your model converges to brilliance or spirals into mediocrity. In this blog, we'll walk through how these components interact—exploring optimizers like Adam, SGD, and RMSProp, various learning rate schedulers, and how they all come together in a modern architecture like MobileNetV3.</p>
      </section>

      <section id="importance">
        <h2>The Importance of Optimization in Deep Learning
        </h2>
        <p>Every deep neural network relies on an optimizer to adjust its internal weights. Optimizers use gradients to figure out in which direction the model's parameters should change to reduce error. However, that path to improvement isn't always linear. It's often filled with sharp turns, local minima, and noisy signals. That's where learning rate schedulers come in.</p>
        <p>The learning rate determines how big each step is in the optimization process. A scheduler dynamically adjusts this rate—starting fast and slowing down for precision or cycling it to escape suboptimal zones. Without a smart strategy, your model could either overshoot good solutions or get stuck in poor ones.</p>
      </section>

      <section id="optimizers">
        <h2>Meet the Optimizers</h2>
        <p><strong>SGD:</strong> The most basic and widely-used optimizer, SGD updates model weights using small batches of data. While it's fast and effective in simpler models, it can be unstable in complex ones. It benefits significantly from additional tweaks like momentum or adaptive learning rate scheduling</p>
        <p><strong>RMSProp:</strong> Designed to stabilize training, RMSProp uses an adaptive learning rate for each parameter, scaling updates according to how frequently a parameter has been updated. It's especially effective for non-stationary objectives, where the data distribution may change over time.</p>
        <p><strong>Adam:</strong> Adam combines the best of RMSProp and momentum-based updates. It tracks the moving average of gradients (first moment) and the squared gradients (second moment), giving it both speed and stability. It's often the go-to optimizer for complex architectures and noisy data.</p>
      </section>

      <section id="schedulers">
        <h2>The Scheduler Squad</h2>
        <ul>
          <li><strong>Triangle:</strong> Learning rate ramps up and down, keeping things fresh.</li>
          <li><strong>Exponential Decay:</strong> Fast at first, slows down as the model matures — wise and steady.</li>
          <li><strong>Warm Restarts:</strong> Presses reset at intervals, like power naps for your optimizer.</li>
          <li><strong>Cosine Annealing:</strong> Smooth, gentle changes — great for avoiding learning spikes.</li>
        </ul>
      </section>

      <section id="mobilenet">
        <h2>Why MobileNetV3?</h2>
        <p>MobileNetV3 is a lightweight neural network designed for efficiency on mobile and embedded devices. It strikes a careful balance between performance and computational cost. It builds upon its predecessors with:

            Inverted Residual Blocks
            
            Squeeze-and-Excite Modules
            
            H-Swish activation function
            
            MobileNetV3 is often used in image classification tasks where speed and size matter, like object detection on smartphones or edge devices.
            
            In our research, MobileNetV3 was trained on the CIFAR-100 dataset—a challenging image classification benchmark with 100 classes. The model was fine-tuned using pre-trained weights from ImageNet V1.</p>
      </section>

      <section id="results">
        <h2>So, What Worked Best?</h2>
        <ul>
          <li><strong>Adam + Triangle2:</strong> Best training performance at 73.2%</li>
          <li><strong>RMSProp + Exponential:</strong> Best generalization with 74.1% test accuracy</li>
          <li><strong>SGD + Cosine:</strong> Stable and smooth, with minimal overfitting</li>
          <p>Interestingly, models trained without a scheduler consistently underperformed, reinforcing how critical learning rate management is to successful training.</p>
        </ul>
      </section>

      <section id="conclusion">
        <h2>Conclusion</h2>
        <p>Training a deep learning model isn't just about feeding it data and letting it run. It's a careful orchestration of choices—which optimizer, which scheduler, what architecture. This research shows how profound the impact of such choices can be. The right combination can unlock better generalization, faster convergence, and reduced overfitting.

            If you're working with complex datasets or deploying models on limited-resource environments like mobile devices, pairing a solid architecture like MobileNetV3 with adaptive optimizers and schedulers can make all the difference.</p>
      </section>

      <footer class="site-footer">
        <div class="footer-container">
          <p class="footer-left">© 2025 Aryan Ankolekar.</p>
          <div class="footer-icons">
            <a href="mailto:aryan.ankolekar@gmail.com" target="_blank"><i class="fas fa-envelope"></i></a>
            <a href="https://github.com/AryanAnkolekar" target="_blank"><i class="fab fa-github"></i></a>
            <a href="https://linkedin.com/in/aryan-ankolekar" target="_blank"><i class="fab fa-linkedin"></i></a>
            <a href="https://twitter.com/AryanAnkolekar" target="_blank"><i class="fab fa-twitter"></i></a>
          </div>
        </div>
      </footer>
    </div>
    
    <aside class="fixed-toc">
      <h3>Sections</h3>
      <ul>
        <li><a href="#intro">Introduction</a></li>
        <li><a href="#importance">The Importance of Optimization</a></li>
        <li><a href="#optimizers">Meet the Optimizers</a></li>
        <li><a href="#schedulers">The Scheduler Squad</a></li>
        <li><a href="#mobilenet">Why MobileNetV3?</a></li>
        <li><a href="#results">So, What Worked Best?</a></li>
        <li><a href="#conclusion">Conclusion</a></li>
      </ul>
    </aside>
  </div>
</body>
</html>
