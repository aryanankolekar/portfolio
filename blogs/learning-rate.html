<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>CP191124 | Aryan Ankolekar</title>
    <link rel="icon" type="image/png" href="../f1.png" />
    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css"
    />
    <link rel="stylesheet" href="styles.css" />
    <link
      href="https://fonts.googleapis.com/css2?family=Nixie+One&display=swap"
      rel="stylesheet"
    />
    <style>
      .blog-tags {
        font-family: "Nixie One", cursive;
        font-size: 1.2em;
      }
      .tag {
        border: none;
        text-decoration: none;
        padding: 0;
        margin: 0 0.5em;
        background: none;
        color: var(--text-color);
        transition: color 0.3s ease;
      }
      .tag:hover {
        text-decoration: none;
        color: var(--accent-color);
      }
    </style>
  </head>
  <body>
    <div class="page-wrapper">
      <div class="main-content">
        <header>
          <h1>
            <div class="header-title">
              <img
                src="../f1.png"
                alt="favicon"
                class="favicon"
                width="5000"
                height="5000"
              />
              <span>Aryan Ankolekar</span>
            </div>
            <div class="header-buttons">
              <button class="search-toggle" aria-label="Open search">
                <i class="fas fa-search"></i>
              </button>
              <button class="theme-toggle" aria-label="Toggle theme">
                <i class="fas fa-moon"></i>
              </button>
            </div>
          </h1>
          <div class="nav-links">
            <a href="/">home</a>
            <a href="/about">about</a>
            <a href="/blogs">blog</a>
            <a href="/blogs/case-study">aps domain</a>
          </div>
        </header>

        <div class="blog-tags">
          <a href="/blogs/tags"><i class="fas fa-tag"></i></a>
          <a href="/blogs/tags/machine-learning" class="tag"
            >#machinelearning</a
          >
          <a href="/blogs/tags/deep-learning" class="tag">#deeplearning</a>
        </div>

        <section id="title">
          <h1 class="text-4xl font-bold text-yellow-600 mb-6">
            CP191124. The Art of Making Models Learn: Optimizers, Schedulers &
            MobileNetV3
          </h1>
          <p>
            At its core, training a neural network means minimizing a loss
            function. This loss measures how far off your predictions are from
            the actual labels. The goal of an optimizer is to adjust the model's
            parameters (weights and biases) in a way that this loss is
            minimized.
          </p>
        </section>

        <section id="introduction">
          <h2>Introduction</h2>
          <p>
            Training deep learning models isn't just about throwing data at a
            neural network and waiting for magic. It's a finely tuned balance of
            algorithms, parameters, and timing. Among these, optimizers and
            learning rate schedulers play a crucial role in determining whether
            your model converges to brilliance or spirals into mediocrity. In
            this blog, we'll walk through how these components
            interact—exploring optimizers like Adam, SGD, and RMSProp, various
            learning rate schedulers, and how they all come together in a modern
            architecture like MobileNetV3.
          </p>
        </section>

        <section id="the-importance-of-optimization">
          <h2>The Importance of Optimization in Deep Learning</h2>
          <p>
            Every deep neural network relies on an optimizer to adjust its
            internal weights. Optimizers use gradients to figure out in which
            direction the model's parameters should change to reduce error.
            However, that path to improvement isn't always linear. It's often
            filled with sharp turns, local minima, and noisy signals. That's
            where learning rate schedulers come in.
          </p>
          <p>
            The learning rate determines how big each step is in the
            optimization process. A scheduler dynamically adjusts this
            rate—starting fast and slowing down for precision or cycling it to
            escape suboptimal zones. Without a smart strategy, your model could
            either overshoot good solutions or get stuck in poor ones.
          </p>
        </section>

        <section id="meet-the-optimizers">
          <h2>Meet the Optimizers</h2>
          <p>
            <strong>SGD:</strong> The most basic and widely-used optimizer, SGD
            updates model weights using small batches of data. While it's fast
            and effective in simpler models, it can be unstable in complex ones.
            It benefits significantly from additional tweaks like momentum or
            adaptive learning rate scheduling
          </p>
          <p>
            <strong>RMSProp:</strong> Designed to stabilize training, RMSProp
            uses an adaptive learning rate for each parameter, scaling updates
            according to how frequently a parameter has been updated. It's
            especially effective for non-stationary objectives, where the data
            distribution may change over time.
          </p>
          <p>
            <strong>Adam:</strong> Adam combines the best of RMSProp and
            momentum-based updates. It tracks the moving average of gradients
            (first moment) and the squared gradients (second moment), giving it
            both speed and stability. It's often the go-to optimizer for complex
            architectures and noisy data.
          </p>
        </section>

        <section id="the-scheduler-squad">
          <h2>The Scheduler Squad</h2>
          <ul>
            <li>
              <strong>Triangle:</strong> Learning rate ramps up and down,
              keeping things fresh.
            </li>
            <li>
              <strong>Exponential Decay:</strong> Fast at first, slows down as
              the model matures — wise and steady.
            </li>
            <li>
              <strong>Warm Restarts:</strong> Presses reset at intervals, like
              power naps for your optimizer.
            </li>
            <li>
              <strong>Cosine Annealing:</strong> Smooth, gentle changes — great
              for avoiding learning spikes.
            </li>
          </ul>
        </section>

        <section id="why-mobilenetv3">
          <h2>Why MobileNetV3?</h2>
          <p>
            MobileNetV3 is a lightweight neural network designed for efficiency
            on mobile and embedded devices. It strikes a careful balance between
            performance and computational cost. It builds upon its predecessors
            with: Inverted Residual Blocks Squeeze-and-Excite Modules H-Swish
            activation function MobileNetV3 is often used in image
            classification tasks where speed and size matter, like object
            detection on smartphones or edge devices. In our research,
            MobileNetV3 was trained on the CIFAR-100 dataset—a challenging image
            classification benchmark with 100 classes. The model was fine-tuned
            using pre-trained weights from ImageNet V1.
          </p>
        </section>

        <section id="so-what-worked-best">
          <h2>So, What Worked Best?</h2>
          <ul>
            <li>
              <strong>Adam + Triangle2:</strong> Best training performance at
              73.2%
            </li>
            <li>
              <strong>RMSProp + Exponential:</strong> Best generalization with
              74.1% test accuracy
            </li>
            <li>
              <strong>SGD + Cosine:</strong> Stable and smooth, with minimal
              overfitting
            </li>
            <p>
              Interestingly, models trained without a scheduler consistently
              underperformed, reinforcing how critical learning rate management
              is to successful training.
            </p>
          </ul>
        </section>

        <section id="conclusion">
          <h2>Conclusion</h2>
          <p>
            Training a deep learning model isn't just about feeding it data and
            letting it run. It's a careful orchestration of choices—which
            optimizer, which scheduler, what architecture. This research shows
            how profound the impact of such choices can be. The right
            combination can unlock better generalization, faster convergence,
            and reduced overfitting. If you're working with complex datasets or
            deploying models on limited-resource environments like mobile
            devices, pairing a solid architecture like MobileNetV3 with adaptive
            optimizers and schedulers can make all the difference.
          </p>
        </section>

        <footer class="site-footer">
          <div class="footer-container">
            <p class="footer-left">© 2025 Aryan Ankolekar.</p>
            <div class="footer-icons">
              <a href="mailto:aryan.ankolekar@gmail.com" target="_blank"
                ><i class="fas fa-envelope"></i
              ></a>
              <a href="https://github.com/AryanAnkolekar" target="_blank"
                ><i class="fab fa-github"></i
              ></a>
              <a href="https://linkedin.com/in/aryan-ankolekar" target="_blank"
                ><i class="fab fa-linkedin"></i
              ></a>
              <a href="https://twitter.com/AryanAnkolekar" target="_blank"
                ><i class="fab fa-twitter"></i
              ></a>
            </div>
          </div>
        </footer>
      </div>

      <aside class="fixed-toc">
        <h3>Sections</h3>
        <ul>
          <li><a href="#intro">Introduction</a></li>
          <li>
            <a href="#the-importance-of-optimization"
              >The Importance of Optimization</a
            >
          </li>
          <li><a href="#meet-the-optimizers">Meet the Optimizers</a></li>
          <li><a href="#the-scheduler-squad">The Scheduler Squad</a></li>
          <li><a href="#why-mobilenetv3">Why MobileNetV3?</a></li>
          <li><a href="#so-what-worked-best">So, What Worked Best?</a></li>
          <li><a href="#conclusion">Conclusion</a></li>
        </ul>
      </aside>
    </div>
    <div id="searchModal" class="search-modal" style="display: none">
      <div class="search-modal-content">
        <button class="close-search" aria-label="Close search">Close</button>
        <div class="search-bar">
          <input
            type="text"
            id="searchInput"
            placeholder="Search articles or tags..."
          />
          <button
            id="clearSearchInput"
            aria-label="Clear search input"
            style="display: none"
          >
            &times;
          </button>
        </div>
        <div class="search-results-container">
          <div class="search-tags" id="searchTags">
            <!-- Tags will be dynamically inserted here -->
          </div>
          <div class="search-articles" id="searchArticles">
            <!-- Articles will be dynamically inserted here -->
          </div>
        </div>
      </div>
    </div>
    <script src="../theme.js"></script>
    <script src="../script.js"></script>
    <button
      id="scrollToNextSection"
      class="scroll-button"
      aria-label="Scroll to next section"
    >
      <i class="fas fa-chevron-down"></i>
    </button>
  </body>
</html>
